---
title: "Notes"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output:
  html_document:
    toc: true
    toc_float: TRUE
    theme: spacelab
---

```{r,echo=F, message=F}
library(dplyr)
library(kableExtra)
```

# About

These are class notes and R code for Professor Ge Zhao's STAT-464 : Applied Regression Analysis for Fall term 2021 at Portland State University. 

# W1-D1 : Mon. Sept. 27, 2021

- will ask around for a bigger room 

- assume we have basic understanding of R 

## Introduction : Chapter 1 

- We want to know if the data matches (1-1), (1-Many), (Many - Many) and such 

- We call the data we are interested in the response $Y$ or sometimes the independet variable 

  - want to see what happens to y and the change of Y (predict Y)
  
  - examples : price of bitcoin, or price of house 

- We call the $X$ the predictor(s)

- want to use prediction to predict a response 

  - view of the model 
  
  - annalyze a model 
  
  - example (bitcoin) : day 1 = \$50, day 2 = \$100, day 3 = \$1,000
  
- math model is a statistical model, and to firgue out $Y=f(x)+\epsilon$ where $\epsilon$ is random error

  - in math model everything is fixed except for the $\epsilon$
  
  - this is a very general problem (tells almost nothing)
  
```{r, echo=F}
X <- c("continuous", "categrorical : ordinal data and nominal data")
Y <- c("continuous", "categorical : ordinal data and nominal data")

data.frame(X,Y) %>% kable() %>% kable_paper
```
- ordinal data (can be ordered) and nominal data (different types of data)"

- in regression class most of the time we are focuses on X=Y : Continuous vs Continuous 

- in experimental design we are focused on Y=X : Continuous vs Categorical 

- special case (in later chapters) Y=X : Ordinal vs. continuous 

- later (in other classes) we may see (double line) experimental design looks at every combination of categorical vs. categorical .

### Summary / Thoughts

We are focusing on data, and relations between $X$ and $Y$. 

Regressions would have continuous vs. continuous. This reminds me graphically of physics problems. How far does something go in a certain amount of time. 

Experimental design would have a continuous $Y$ vs. a Categorical $X$. Graphically I think of histograms. 

Some of the special cases I would need to think about. 

# W1-D2 : Wed. Sept. 29, 2021

- If you cannot attend in person use zoom number in D2L link 

### Simple Linear Regression Model 

A model with a single regressor (predictor) x that has a relationship with a response y that is a straight line line. $$y=\beta _{0}+\beta _{1}x+\epsilon$$

- $x$ is the data, $\beta _{0}$ is the intercept, $\beta _{1}$ is the slope, and $\epsilon$ is a random error component. 

  - $\beta$ unknown constants (parameters)
  
  - x is independent variable (predictor or regressor) and y is dependent variable (response).
  
  - simple because there is only one regressor (x).
  
  - the slope $\beta_{1}$ is the change in the mean of the distribution of y produced by a unit change in x.
  
  - example might be $x_1$ = pulse rate (rest rate), and $y$ might be oxygen consumption rate (oxygen rate)

- data points don't fall on a straight line so we can quantify the distances between each point and the best fit line

- $\epsilon$ is assumed to have an expectation or mean of 0 and an unknown variance $\sigma ^{2}$.

Why can you make the assumption that the expectation of $\epsilon$ is zero? 

- The expectation of a random variable must be a fixed random variable (number) whose derivative would be 0.

- can always absorb +c into $\beta$

- $\epsilon _{i}$ are uncorrelated from observation to observation 

### Mean and Variance of SLR

$$E(y|x)=\mu_{x|y}=E(\beta _{0}+\beta _{1}x+\epsilon)=\beta _{0}+\beta_{1}x$$

- $E(y|x)$ referes to the mean _y_ conditional on a specific value of _x_. 

$$Var(y|x)=\sigma^{2}_{y|x}=Var(\beta _{0}+\beta _{1}x+\epsilon)=\sigma^{2}$$

- homogenous variance assumption 

### Least-Square Estimation 

Suppose we have $n$ pairs of data : $(y_{1},x_{1}),(y_{2},x_{2}),...,(y_{n},x_{n})$. Then for a single point $$y_{i}=(\beta _{0}+\beta _{1}x+\epsilon_{i})$$

<center>

OR

</center>

$$\epsilon_{i}=y_{i}-\beta _{0}-\beta _{1}x$$

The idea is to minimize variations (the size of $\epsilon_{i}$), by taking the minimum : 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

$$S(\beta_{0},\beta{1})=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

- when the value is squared it will not be canceled with itself 

Optimize by taking the derivative, set that to zero, and solve

$\Rightarrow$ Take partial derivatives

$$\frac{\partial S}{\partial \beta_{0}}=\sum_{i=1}^{n}-2[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$$\frac{\partial S}{\partial \beta_{1}}=\sum_{i=1}^{n}-2x_{i}[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$\Rightarrow$ set both equations equal to 0

$$\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\beta _{0}-\sum_{i=1}^{n}\beta _{1}x_{i}=0$$

$$\sum_{i=1}^{n}y_{i}x_{i}-\sum_{i=1}^{n}\beta _{0}x_{i}-\sum_{i=1}^{n}\beta _{1}x_{i}^{2}=0$$

### Summary 

- Class notes are difficult to see and hear, but follow the online notes from another student from a past term. Will try to stay in front of reading both textbook and old notes before class period. Will need to spend some time everyday on this class. 

- Data = Model + Error , where error is a deviation from the model 

- Discussed Simple Linear Regression Model, Expectation (mean), and variance, as well as the least squares estimate

- $\beta_{0}$ is the intercept, and $\beta_{1}$ is the slope

# W1-D3 : Fri. Oct. 1st, 2021

### Properties of least squares estimates

Today we will prove ??? 


This is the SSR form 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$



$\overline{xy}$ is **not** $x*y$ but the area


- rewrite solutions :
  - cancle out n's
  - multiple $\overline{x}^{2}$  
  - cancle the terms with beta 0 
  
$\beta_{1}\{\overline{x}^2-\{\overline{x}\}^{2}\}=\overline{xy}-\overline{x} \cdot\overline{y}$

From (1) we can have relationship $\beta_{0}=\overline{y}-\beta_{1}\overline{x}$


...

12:03

$\hat{\beta}=\frac{\overline{xy}-\overline{x} \cdot\overline{y}}{\overline{x}^{2}-\{\overline{x}\}^{2}}=\frac{\sum}{\sum}$

### Summary

- Trying to follow along with proofs that I can't see isn't working. 

- If time this term, it wouldn't hurt to revist these lectures and try to fill out these notes. 

- [October 1](http://web.pdx.edu/~gibson25/564/564-lecture-notes.html#12_-_october_1,_2020) should be the proof we did today. 

# W2-D4 : Mon. Oct. 4th, 2021

### R code lm()

lm(response ~ predictor, data = data)

- next chapter we will have several precidtors (where we will use +)

- lm() includes most of the important information and bottom part tells us how good our model is (Residual s.e., Multiple R-squared, F-statistic)

Final / Midterm will have coding at the end (like appendix), but in homework keep code output and explanation answering the question

### Properties of least squares estimates

### 1. least square estimates are linear combinations of y_i

$$\hat{\beta}_1=\frac{\sum\limits_{i=1}^n(x_i-\overline{x})y_i}{\sum\limits_{i=1}^n(x_i-\overline{x})^2}$$

$$\hat{\beta}_0=\overline{y}-\overline{x}\cdot\hat{\beta}_1$$

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of $y_{i}$'s

\begin{equation}\label{beta_1 linear to y}
\begin{split}
\hat{\beta}_1 & = \frac{\sum\limits_{i=1}^n(x_i-\overline{x})y_i}{\sum\limits_{i=1}^n(x_i-\overline{x})^2}\\
& = \sum\limits_{i=1}^n\frac{(x_i-\overline{x})}{\sum\limits_{i=1}^n(x_i-\overline{x})^2}y_i\\
& = \sum\limits_{i=1}^n\omega_iy_i\quad\text{where }\omega_i=\frac{(x_i-\overline{x})}{\sum\limits_{i=1}^n(x_i-\overline{x})^2}
\end{split}
\end{equation}

- can think of as a weight 

- don't care about x because it has a term, but y is random from epsilon 

\begin{equation}\label{beta_o linear to y}
\begin{split}
\hat{\beta}_0 & = \hat{\beta}_0=\overline{y}-\overline{x}\cdot\hat{\beta}_1\\
& = \frac{1}{n}\sum\limits_{i=1}^ny_i-\overline{x}\cdot\sum\limits_{i=1}^n\omega_iy_i\\
& = \sum\limits_{i=1}^n(\frac{1}{n}-\overline{x}\omega_i)y_i
\end{split}
\end{equation}

### 2. least squares estimators are unbiased of parameters 

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$ 

- the expectation of $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbias estimators 

  - meaning if we repeat many times then the average will be the truth 

- this means estimators are good because it wont be far from the truth 

### Proof that expectations of estimators equals parameters 

We need to prove that $E[\hat{\beta}_{0}]=\beta_{0}$ and $E[\hat{\beta}_{1}]=\beta_{1}$

First assume that $E[\hat{\beta}_{1}]=\beta_{1}$. 

- we are using property number 1 becase if the expectation is a linear function then constants can move inside and outside expectation : $E(a\cdot X+b\cdot Y)=aE(X)+bE(Y)$

- note we normally we use lower case letter for numbers we know and upper case for those which are random. The y below is capitalized because it is random and unknown (because of epsilon). 

\begin{equation}\label{estimators expectation proof}
\begin{split}
E[\hat{\beta}_{1}] & = E{\sum\limits_{i=1}^n\omega_iY_i}\\
& = \sum\limits_{i=1}^n\omega_iE(Y_i)\\
& = \sum_{i=1}^{n}\omega_i(\beta_{0} +\beta_{1}x_{i})\\
& = \sum_{i=1}^{n}\omega_i\beta_{0}+\sum_{i=1}^{n}\omega_i\beta_{1}x_{i}\\
& = \beta_0\sum_{i=1}^{n}\omega_i+\beta_{1}\sum_{i=1}^{n}\omega_ix_{i}\\
& = 0 + \beta_1\cdot 1\\
& = \beta_1
\end{split}
\end{equation}

Therefore by the proof of equalities $E[\hat{\beta}_{1}]=\beta_{1}$. QED. 

Note : $\omega_i=\frac{(x_i-\overline{x})}{\sum\limits_{i=1}^n(x_i-\overline{x})^2}=\frac{1}{\sum\limits_{i=1}^n(x_i-\overline{x})^2}\sum\limits_{i=1}^n(x_i-\overline{x})$ (same form as last Friday)

- this is tricky, but key part of linear model 

Also Remember 

- y : not random, Y : random 

- cannot assume x to be random because it is apart of the observed data 

### Summary 

Properties of Least Squares Estimators: 

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of $y_{i}$

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$

- Which means : $E[\hat{\beta}_{0}]=\beta_{0}$ and $E[\hat{\beta}_{1}]=\beta_{1}$

# W2-D5 : Wed. Oct. 6th, 2021

- there is a 300 level version of this course  . 

- next we show we can compute the variance of $\hat{\beta}_1$ and $\hat{\beta}_0$. 

### Variance of least squares estimators

3. The variance of $\hat{\beta}_1$ and $\hat{\beta}_0$

**<span style="color: violet;">Assumptions</span>**

$E(\epsilon )=0$

$Var(\epsilon)=\sigma^2$

$\epsilon_i$ and $\epsilon_j$ are independent. ($\epsilon_i \ne \epsilon_j$)

$E(\epsilon_i\cdot\epsilon_j)=0$

$Cov(\epsilon_i\cdot\epsilon_j)=0$

Note : the only term with randomness is $\hat{y}$

### Variance of least squares estimates

\begin{equation}\label{var of slope estimate}
\begin{split}
Var[\hat{\beta}_{1}] & = Var\{\sum_{i=1}^{n}\frac{x_{i}-\overline{x}}{S_{xx}}Y_{i}\}^2\\
& = \sum_{i=1}^{n}Var\{\frac{x_{i}-\overline{x}}{S_{xx}}Y_{i}\}^2+0(\text{Cov.})\\
& = \sum_{i=1}^{n}\{\frac{x_{i}-\overline{x}}{S_{xx}}\}^{2}Var(Y_{i})^2\\
& = \sum_{i=1}^{n}\{\frac{x_{i}-\overline{x}}{S_{xx}}\}^{2}\sigma^2\\
& = \frac{\sigma^2}{S_{xx}^2}\sum_{i=1}^{n}(x_{i}-\overline{x})
\end{split}
\end{equation}

\begin{equation}\label{var of intercept estimate}
\begin{split}
Var[\hat{\beta}_{0}] & = Var(\overline{Y}-\hat{\beta}_1\overline{x})\\
& = Var\{\frac{1}{n}\sum_{i=1}^{n}Y_{i}-\overline{x}\sum_{i=1}^{n}\frac{x_{i}-\overline{x}}{S_{xx}}Y_i\}^2\\
& = Var[\sum_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}Y_i]^2\\
& = \sum_{i=1}^{n}Var\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2Y_i\\
& = \sum_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2\sigma^2
\end{split}
\end{equation}

- remember that $\beta_{0}$ is y intercept 

### Sum of residuals is zero

4. $e_{i}=y_i+\hat{y}_i$

> $\sum_{i=1}^{n}e_i=0$
> 
> $\sum_{i=1}^{n}u=\hat{y}_i$

- proof is simple (and will be added to homework) : just plug in $\hat{y}_i$ and things will cancel 

- make sure to show results of one to be true before using it to prove the other

### Least Squares goes through the centroid of the data

5. $\overline{y}=\hat{\beta}_0+\hat{\beta}_1\overline{x}$

- centroid is $(x_i,y_i)_{i=1}^n$

- proof is simple just plug in estimator 

### LS Weighted by the corresponding fitted value equals zero 

6. $\sum_{i=1}^{n}x_ie_i=0$ and $\sum_{i=1}^{n}\hat{y}_ie_i=0$

> $\sum_{i=1}^{n}x_ie_i=0$
>
> $\sum_{i=1}^{n}\hat{y}_ie_i=0$

- need to expand to prove

- will be additional homework problem 

- residual is particular to x

- residual is particular to $\hat{y}$

- $x_i$ and $\hat{y}_i$ are 1 dimennsional vectors 

### Is this the correct line? 

- Original problem : When we assume a linear model how do we know we are using the correct line? 

- Is our assumption correct? 

- Is the model useful? 

- We do not know the parameters ($beta_0$ and $\beta_1$) (intercept and slope) so we estimated them 

- The answer will be an inference, but before we can make that inference we neet to estimate $\sigma^2$

- In most real life cases we do not know $\sigma^2$

### Estimating sigma squared 

Recall $e=y-\hat{y}$ 

$\sigma^2=Var(\epsilon)$

$\epsilon = Y -(\beta_0+\beta_1x)$

$e=y-(\hat{\beta}_0+\hat{\beta}_1x)=y-\hat{y}$

- idea is that we use the residuals to estimate the variance of error

Lets define these two quantities : 

$SS_{Res}=\sum\limits_{i=1}^{n}e^2_i=\sum\limits_{i=1}^{n}(y-\hat{y})^2$

$E(\sum\limits_{i=1}^{n}e^2)=(n-2)\sigma^2$

$\Rightarrow \sigma^2=\frac{E(\sum\limits_{i=1}^{n}e^2)}{n-2}$

- we don't know the true sigma, so we use this quantity to estimate sigma squared 

$\hat{\sigma}^2=\frac{\sum\limits_{i=1}^{n}e^2}{n-2}$

- because if we take the expectation of both sides we will have sigma squared 

$E(\hat{\sigma}^2)=E(\frac{\sum\limits_{i=1}^{n}e^2}{n-2})=\sigma^2$

- will be used a lot in the future

- note that this estimate is model dependent 

- in textbook as $MS_{\text{Res}}$ and sometimes called residual mean square

- square root of sigma squared sometimes called standard error of regression

### Summary 

- homework due monday 

- skip problems that deal with confidence interval 

- will go over some of the coding in class

- There are 6 properties of our least squares estimators $\hat{\beta}_0$ and $\hat{\beta}_1$

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of of the observations $y_{i}$

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$ 

3. Variance of LS estimates $\hat{\beta}_0$ and $\hat{\beta}_1$

- $Var[\hat{\beta}_{1}]=\frac{\sigma^2}{S_{xx}^2}\sum\limits_{i=1}^{n}(x_{i}-\overline{x})$

- $Var[\hat{\beta}_{0}]=\sum\limits_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2\sigma^2$

4. Sum of residuals is zero

- $e_{i}=y_i+\hat{y}_i$

- $\sum\limits_{i=1}^{n}e_i=0$
 
- $\sum\limits_{i=1}^{n}u=\hat{y}_i$

5. LS goes through the centroid of the data

- $\overline{y}=\hat{\beta}_0+\hat{\beta}_1\overline{x}$

6. LS, Weighted by the corresponding fitted value, equals zero 

- $\sum\limits_{i=1}^{n}x_ie_i=0$ and $\sum_{i=1}^{n}\hat{y}_ie_i=0$

Estimate sigma^2 : $\hat{\sigma}^2=\frac{\sum\limits_{i=1}^{n}e^2}{n-2}$

# W2-D6 : Fri. Oct. 8th, 2021

- will go over coding in R 

- update R studio and packages 

- plan B : just use code in R (instead of Rmd) ... ?

- will upload html to D2L

- everything covered in class is covered in code (gray boxes)

- want to double check result with correct answer look for white boxes

- no actual help on homework, but we will see what happens I guess

### Hypothesis Testing 

- $\hat{y}=\hat{\beta}_0-\hat{\beta}_1\overline{x}$ is the estimated Y with estimated parameters 

- Why wouldn't our professor believe this : $Y=\beta_0+\beta_1x+\epsilon$, but would beleive this : $Y=\epsilon$?

- Recall that E($\epsilon$)=0 and VAR($\epsilon$)=$\sigma^2$ are unknown. 

- How do we know which Y is correct? 

- What is the difference between $\beta_1=0$ and $\beta_0=0$ OR $\beta_1\ne0$ and $\beta_0\ne0$

- If $\beta_1$ is zero then x shouldn't be included in the model. 

- $H_0=\beta_1=0$, $\beta_0=0$

- $H_0=\beta_1\ne0$, $\beta_0\ne0$

Question 1 : Do we really need x to explain y? 

- in math language : $\beta_1=0$

... to be continued next class. 

### Summary 

- finish homework (complete)

# W3-D7 : Mon. Oct. 11th, 2021

- 2 proofs are due for homework 2 

### Section 2.3 Hypothesis testing on parameters 

- $H_0$ : $\beta_1=0$ and $H_1$ : $\beta_1\ne0$

- we want to test if a statement is true or not 

- if $H_0$ is true then our model : $Y=\beta_0+\beta_1x+\epsilon$ has nothing to do with x. 

  - meaning that Y is just a constant plus $\epsilon$
  
  - can't get any information from model 
  
  - this is a very strong statement 
  
  - and our testing problem 
  
### t-statistic

- Recall $\epsilon_i\sim N(0,1)$ and that $\epsilon_i$ and $\epsilon_j$ are independent with $i\ne j$

- If we have the assumption that $\epsilon_i$ is independent and identically distributed, then 

$$t=\frac{\hat{\beta}_1-\hat{\beta}_{10}}{se(\hat{\beta}_{10})\}}\sim t_{n-1}$$
  
- this ratio is the t statistic exzactly

- n-2 because there are two parameters 

- later there will 3 up to p parameters

- Proof for this depends on $\chi^2$ (chi squared distribution) 

- se is squared root varience 

- Initially we assume that $H_0$ is true ($\beta_1=0$), but if z value is large then $H_1$ may be false

- $t=\frac{\hat{\beta}_1-\hat{\beta}_{10}}{se(\hat{\beta}_{10})}$ is called the "tester statistic" 

  - if $H_0$ is true then t is close to 0
  
  - if $H_0$ is false then t is further away (closer to 1)
  
![](img/img1.webp)

- img. above is pdf of t-distribution 

- p-value = 2(area1) = 2(area2) = $2Pr(T>t)=Pr(T>|t|)$

- p value is the probability (between 0 and 1) that $H_0$ is true.

- a p-value that is < significance level (usually 0.05), the we reject $H_0$. 

  - (accepting $H_1$)
  
  - example : P value of 0.00000000005 we can reject 
  
- a p-value that is > significance level (usually 0.05), then we fail to reject $H_0$.

  - not deterministic result 
  
  - note p-value is only the probability
  
  - if we accept $H_1$ then we say it is true, which is not the case
  
  - example : P value of 0.25 we fail to reject 
  
- consider p-value as uncertainty 
  
### Special Case (sigma squared is known)

- $\sigma^2$ is known (happens in some homework problems)

- "ideal case" 

- $t=\frac{\hat{\beta}_1-\hat{\beta}_{10}}{se(\hat{\beta}_{10})\}}\sim N(0,1)$

### One sided Test 

- A two sided test is what we were looking at before. 

- Often Written : $H_0$ : $\beta_0=\beta_{10}$ vs. $H_1$ : $\beta_0\ne\beta_{10}$

- A one sided test is : $H_0$ : $\beta_0=\beta_{10}$ vs. $H_1$ : $\beta_0>\beta_{10}$

- if test is one sided, then it will depend which side the p-value is on 

- only one side is being counted ($\frac{1}{2}$ Area)

### Summary 

- will revisit t (and maybe prove in a different form)

- update code page

- create homework page? 

- fill in holes in notes 

- finish chapter 2 

- peek at chapter 3

# W3-D8 : Wed. Oct. 13th, 2021

- this next section follows the book (something to do with $R^2$)

### Analysis of Variance 

$\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$

- SSTO = Sum Square Total

- when we calculate simple variance of $y_i$ we use this formula 

- sometimes we might have a $\frac{1}{n}$ or some other constant in front of the sigma, but we can forget about that 

- the data point ($y_i$) minus the center point ($\overline{y}$)

  - the distance between the point and data mean (visualize graphically)
  
- we will decompose this into two parts 

$\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{n}(\hat{y}_i-\overline{y})^2$

- the first part is the residual : $\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2$

  - SSE : Sum Square of Error
  
  - if our model perfectly fits the data then $\hat{y}$ will be $y_i$ and residual will be zero
  
  - $SS_{RE}$

- the second part is variability of fitted data (SSR): $\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$

  - $\overline{\hat{y}}=\overline{y}$ : average of estimated y is the same as the original value 
  
  - SSR : Sum Square of Regression 
  
  - $SS_{Reg}$
  
- everything is decomposing this : $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$ variance
  
  - this means that the variance of the data ($\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$) can be explained by the variance of the error ($\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2$) and the variance of model ($\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$)
  
  ### Proof : Analysis of Variance 
$\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$ is true. 

\begin{equation}\label{analysis of var. proof part a}
\begin{split}
\sum\limits_{i=1}^{n}(y_i-\overline{y})^2 & = \sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2\\
& = \sum\limits_{i=1}^{n}(y_i-\hat{y}_i+\hat{y}_i-\overline{y})^2\\
& = \sum\limits_{i=1}^{n}\{(y_i-\hat{y}_i)^2+(\hat{y}_i-\overline{y})^2+2(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})\}
\end{split}
\end{equation}

What we need to show is that this term : $(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})$ , is zero. 

- $\sum\limits_{i=1}^n(y_i-\hat{y}_i)(\hat{y}_i-\overline{y}) = 0$

\begin{equation}\label{aov lemma}
\begin{split}
\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)(\hat{y}_i-\overline{y}) & = \sum\limits_{i=1}^n(y_i-\hat{y}_i)\hat{y}_i-\sum\limits_{i=1}^n(y_i-\hat{y}_i)\overline{y}\\
& = \sum\limits_{i=1}^ne_i\hat{y}_i-\overline{y}\sum\limits_{i=1}^ne_i\\
& = 0 - 0\\
& = 0
\end{split}
\end{equation}

- note to show that $\sum\limits_{i=1}^ne_i\hat{y}_i=0$ write out each of the individual terms for $\hat{y}_i$ and $e_i$

Therefore $\sum\limits_{i=1}^n(y_i-\hat{y}_i)(\hat{y}_i-\overline{y}) = 0$ , and such $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$

### Degrees of Freedom 

- in probability theory we need to look at the degrees of freedom : df

  - for SSTO :  df = n - 1
  
  - for SSE : df = n-2
  
    - $\hat{\beta}_0$ and $\hat{\beta}_1$

  - for SSR : df = 1

### Decomposing terms : SSE , SSR , MSE 

$1=\frac{SSE+SSR}{SSTO}=\frac{SSE}{SSTO}+\frac{SSR}{SSTO}$

- SSTO is fixed 

- $+\frac{SSR}{SSTO}$ is $R^2$ (we will talk more about $R^2$ when we talk about the confidence interval)

- $\frac{SSE}{SSTO}$ is 

- we use SSE to estimate the variance

  - once we have variance we can show probability, density, or distribustion of almost anything 

$SSE=\frac{n-2}{\sigma^2}\sim x^2_{n-2}$

  - chi squared n - 2 distribution 
  
  - $\frac{SSE}{\sigma^2}=MSE\frac{n-2}{\sigma^2}$

$SSR=1\cdot MSR\sim x_1^2$

  - chi squared 1 distribution 

  - $\frac{SSR}{\sigma^2}=1\cdot \frac{MSR}{\sigma^2}$

$E(MSE)=\sigma^2$

> $MSE=\frac{1}{n-2}\sum\limits_{i=1}^n(y_i-\hat{y}_i)^2$

$MSE=\frac{SSE}{n-2}$

$MSE=\frac{SSE}{\text{df of SSE}}$

$MSR=\frac{SSR}{\text{df of SSR}}$

### F and  T Statistic 

What happens when we take one and divide it by another one? 

- put the smaller one (degrees of freedom) ontop of the larger one

$\sqrt{\frac{MSR}{MSE}}=\frac{1\cdot\frac{MSR}{\sigma^2}}{MSE\frac{n-2}{\sigma^2}}=\frac{MSR}{(???)MSE}=\frac{\chi_1^2/1}{\chi_{n-2}^2/n-2}$

F statistic : $F_{1, n-2}\sim \frac{\chi_1^2/1}{\chi_{n-2}^2/n-2}$

$\frac{\chi^2_m/m}{\chi_n/n}\sim F_{m,n}$ , if $\chi^2_n$ is  (independent) from $\chi^2_m$ then 

  - $\newcommand{\indep}{\perp \!\!\! \perp}\chi^2_n\indep \chi^2_m$
  
We have another problem ... what about T distribution? 

$T\sim df_n$

What is $T^2$ distribution? 

- $T^2\sim F_n$

We can change these : $T\sim df_{n-2}$ and $T^2\sim F_{n-2}$

### Key Points

- The analysis equations is the most critical result 

- we also decompose this equation into many terms, with each term responding to error , regresseion with ... a bunch of stuff. 

- if we find that any term is too small to consider we can remove it (here is only two parts so not very complex)

Test : $\beta_1=0$ vs. $\beta_1\ne0$

### Confidence Interval of parameter

Pr($1,\beta_1<\mu=1-\alpha)=<\alpha\ne1$

- of course we want this probability to be 1

- currently we dont know if this is the number we have or not 

- next we are going to try to connect bact to $1-\alpha$

- probability of random vairable : $\beta_1$ is computed here

$t=\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}\sim t_{n-2}$

- distribution of $\beta_1$ is defined here 

Can we solve for our $\mu$? 

- if we can we can find the interval for $\beta_1$

- we will do this this Friday. 

### Summary 

- analysis of variance : $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{n}(\hat{y}_i-\overline{y})^2$

- SSTO : sum of squares total : $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$

- SSE : sum of square error : $\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2$

- SSR : sum of square regression : $\sum\limits_{i=1}^{n}(\hat{y}_i-\overline{y})^2$

- MSE : mean square error (??) : $MSE=\frac{1}{n-2}\sum\limits_{i=1}^n(y_i-\hat{y}_i)^2$

- F Statistic : $\frac{\chi^2_m/m}{\chi_n/n}\sim F_{m,n}$

- T Statistic : $T^2\sim F_{n-2}$

- Confidence interval for $\beta_1$'s 

  - probaility : Pr($1,\beta_1<\mu=1-\alpha)=<\alpha\ne1$
  
  - distribution : $t=\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}\sim t_{n-2}$
  
- HW 2 is live : Proofs and fill in skipped questions from HW 1

# W3-D9 : Fri. Oct. 15th, 2021

- Problem is that we want to find the confidence interval (CI) for parameters $\beta_0$ and $\beta_1$, and $\sigma^2$

### CI For Beta_0 , Beta_1, and Sigma Squared

$Pr\{l<\beta_1<u\}=1-\alpha$

- we want probability that $\beta_1$ is greater than some lower bound and less than some upper bound is equal to some number, lets call alpha for now

- currently we don't know the lower and the upper 

  - we do know $t=\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}\sim t_{n-2}$ 
  
  - which is a different form of $\beta_1$
  
  - call this ratio t

- we also don't know the distribution of $\beta_1$. If we did we knew that we would know the probability and be able to find the lower and upper bounds. 

\begin{equation}\label{pr beta 1}
\begin{split}
1-\alpha & = Pr\{l<\beta_1<u\}\\
& = Pr\{-u<-\beta_1<-l\}\\
& = Pr\{\hat{\beta}_1-u<\hat{\beta}_1-\beta_1<\hat{\beta}_1-l\}\\
& = Pr\{\frac{\hat{\beta}_1-u}{s.e.(\hat{\beta}_1)}<\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}<\frac{\hat{\beta}_1-l}{s.e.(\hat{\beta}_1)}\}
\end{split}
\end{equation}

- now the probabilty becomes : 

$Pr(\frac{\hat{\beta}_1-u}{s.e.(\hat{\beta}_1)}<t<\frac{\hat{\beta}_1-l}{s.e.(\hat{\beta}_1)})=1-\alpha$

- we know t-distribution, so if we can figure our $\hat{\beta}_1$ and $s.e.(\hat{\beta}_1)$ then we can find upper and lower bounds 

![](img/img2.png)

- we have and even distribution with evenly distanced intevals a and -a 

- we know the area between these two points is $1-\alpha$

- that means the area of ends is $\frac{\alpha}{2}$

- $a=t_{\frac{\alpha}{2},n-2}$ , $t_{1-\frac{\alpha}{2},n-2}$ , $t_{n-2}^{\frac{\alpha}{2}}$

- plug in $a=t_{\frac{\alpha}{2},n-2}$ , then we have 

$\frac{\hat{\beta}_1-u}{s.e.(\hat{\beta}_1)}=-t_{\frac{\alpha}{2},n-2}$

$\Rightarrow u=\hat{\beta}_1+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

$\Rightarrow l=\hat{\beta}_1-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

- put this back in the original equation : $Pr\{l<\beta_1<u\}=1-\alpha$

- If we want a 99% confidence interval then our alpha needs to be 1%

- 95% CI, $\alpha=0.05$

For $Pr\{l<\beta_0<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\hat{\beta}_0+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

$\Rightarrow l=\hat{\beta}_0-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

### CI of Sigma Squared 

$\sigma^2$ ? 

$Pr\{l<\sigma^2<u\}=1-\alpha$

$\frac{(n-2)MSE}{\sigma^2}\sim \chi^2_{n-2}$

- this ratio is chi squared distribution with degrees of freedom n-2. wont prove but will need to know decompositions to prove

To help remember : 

$E(SSE)=(n-2)\sigma^2$

$MSE=\frac{1}{n-2}SSE$

![](img/img3.png)

- we are still looking for some interval c and d such that the blue area is $1-\alpha$. 

- area of a and b are : $a=\chi^2_{\frac{\alpha}{2},n-2}$ and $b=\chi^2_{1-\frac{\alpha}{2},n-2}$

$Pr\{\frac{(n-2)MSE}{u}<\frac{(n-2)MSE}{\sigma^2}<\frac{(n-2)MSE}{l}\}=1-\alpha$

- the lower probability is a and the upper probability is b so, 

$\Rightarrow u=\frac{(n-2)MSE}{\chi^2_{\frac{\alpha}{2},n-2}}$

$\Rightarrow l=\frac{(n-2)MSE}{\chi^2_{1-\frac{\alpha}{2},n-2}}$

- in this course we do not care too much for the CI of $\sigma^2$, but about the CI for $\beta_1$ and $\beta_0$

### Prediction 

- what people really want is the response, the prediction 

model : $Y=\beta_0+\beta_1x+\epsilon$

- $\beta_0+\beta_1x$ : first part is what we can determine (linear part). 

- $\epsilon$ : second part is something random, not in our control.

  - assume normal 

description : $\{x_i,y_i\}^n_{i=1}$

- $y_i=\hat{\beta}_0+\hat{\beta}_1x_i$
 
estimated model : $\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x$

point prediction :  $y_{\text{new}}=\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}+\epsilon_{\text{new}}$

- dont know random unknown term $\epsilon_{\text{new}}$

We can comput the CI of $\hat{\beta}_0$ and $\hat{\beta}_1$

- key part of this is that the $Var(\hat{\beta}_0)$ and $Var(\hat{\beta}_1)$

 Now want the CI of $y_{\text{new}}$
 
- we already know $\hat{y}_{\text{new}}=???$ this is our hat estimate

- next step is $Var(y_{\text{new}})$ 

  - if we can compute the variance of y hat then we can compute our CI
  
###

![](img/img4.png)

- this is a straight line 

- given $x_{\text{new}}$ we can find our prediction on this straight line 

So what is $y_{\text{new}}$

- it should be somewhere along the blue line 

- so we will find an interval that 95% of the data lies within, that we can way with 95% confidence that $y_{\text{new}}$ lies within 

- $Var(y_{\text{new}})$ is so important because it will help us with calculations from earlier in lecture (for upper and lower values : u and l) 

\begin{equation}\label{var of y hat}
\begin{split}
Var\{y_{\text{new}}\} & = Var\{\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}+\epsilon_{\text{new}}\}\\
& = Var\{\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}\}+Var\{\epsilon_{\text{new}}\}\\
& = Var\{\hat{y}\}+\sigma^2
\end{split}
\end{equation}

What you need to do next is write out the formular for $\hat{y}$ and take the square root : $\sqrt{...\cdot\sigma^2}$

- will expand on the details on Monday 

- Homework expanded to Wednesday so we can do all predictions and confidence interval 

### Summary 

- will expand on prediction on monday 

For $Pr\{l<\beta_1<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\hat{\beta}_1+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

$\Rightarrow l=\hat{\beta}_1-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

For $Pr\{l<\beta_0<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\hat{\beta}_0+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

$\Rightarrow l=\hat{\beta}_0-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

For $Pr\{l<\sigma^2<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\frac{(n-2)MSE}{\chi^2_{\frac{\alpha}{2},n-2}}$

$\Rightarrow l=\frac{(n-2)MSE}{\chi^2_{1-\frac{\alpha}{2},n-2}}$

### To Do 

- prep homework 2 (complete)

# W4-D10 : Mon. Oct. 15th, 2021

CI of $\hat{\beta}_1$ , $\hat{\beta}_0$ , $\hat{\sigma}^2$

- once we know how to do the CI of these three then we can do the standard error of $s.e.\hat{\beta}_1$ and $s.e.\hat{\beta}_0$

Next we can find a predictor of y at $x_\text{new}$

$\hat{y}_\text{new}=\hat{\beta}_0+\hat{\beta}_1x_\text{new}$

- not at original x but new x, so new y 

- $\hat{y}_\text{new}$ is an estimation so we don't know how far away it is from $y_\text{new}$

- so we will use the CI to show that within a certain level of confidence the interval with include this 

### CI of y_new

$y_\text{new}=\hat{\beta}_0+\hat{\beta}_1x_\text{new}+\epsilon_\text{new}$

- there is an error term that corresponds to x new

- we don't know exact value of epsilon new

- that means y_new is random, so we can find Var(y_new)

$Var(y_\text{new})=Var(\hat{\beta}_0+\hat{\beta}_1x_\text{new}+\epsilon_\text{new})$

- $E(y_\text{new})=\beta_0+\beta_1x_\text{new}$

- now we have several different situations to solve for var

### 1. Only 1 x_new

1. Only one $x_{\text{new}}$

\begin{equation}\label{var of y hat full}
\begin{split}
Var\{y_{\text{new}}\} & = Var\{\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}\{+Var\{\epsilon_{\text{new}}\}\\
& = \sigma^2\{\frac{1}{n}\frac{(x_{\text{new}}-\overline{x})^2}{S_{xx}}\}+\sigma^2\\
& = \sigma^2\{1+\frac{1}{n}\frac{(x_{\text{new}}-\overline{x})^2}{S_{xx}}\}
\end{split}
\end{equation}

$s.e.(\hat{y}_{\text{new}})=\hat{\sigma}\sqrt{1+\frac{1}{n}\frac{(x_{\text{new}}-\overline{x})^2}{S_{xx}}}$

We use : $\frac{\hat{y}_\text{new}-y_\text{new}}{s.e.(\hat{y}_\text{new})}\sim t_{n-2}$ to create the C.I. 

C.I. of $y_{\text{new}}$ is $\{\hat{y}_{\text{new}}\pm t_{}\cdot \sqrt{1+\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

- with 95% confidence that $y_\text{new}$ is in this interval

### 2. m identical x_new

2. m identical $x_{\text{new}}$

- What if we have many x's? 

$s.e.(\hat{y}_\text{new})=\hat{\sigma}\sqrt{\frac{1}{m}+\frac{1}{n}\frac{(x_{\text{new}}-\overline{x})^2}{S_{xx}}}$

> $Var(\frac{1}{m}\sum\limits_{i=1}^n\epsilon_{new})=\frac{1}{m}\sigma^2$

C.I. of $y_{\text{new}}$ is $\{\hat{y}_{\text{new}}\pm t_{\frac{\alpha}{2},n-2}\cdot \sqrt{\frac{1}{m}+\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

### 3. Infinite many x_new

3. Expected Prediction of $x_\text{new}$

- we are looking for expected y_new

- mean of many y-news

$s.e.(\hat{y}_\text{new})=\hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

- infinite means that m is positive infiniity, if you plug in then the first term is zero 

C.I. of $y_{\text{new}}$ is $\{\hat{y}_{\text{new}}\pm t_{\frac{\alpha}{2},n-2}\cdot \hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

- with 95% that the average of $y_\text{new}$ is in this interval 

![](img/img5.png)

### R

- prediction section in code is "stupid way" 

- triangle is $\hat{y}_\text{new}$ , which is on the straight line

  - C.I. are upper and lower dots
  
  - if expand then we have a confidence band
  
Code : 

function confint()

create new data, plug in data from orignial and assume new x 

function predict()

- assign interval 'prediction' and 'confidence' respectively 

- put them together and have a confidence band 

- wider band is argument 

- dashed curve is confidence of senario 3 

- always smaller than other curves

### Coefficient of Determination (R Squared)

- this is one way to measure how good our model is

$R^2=1-\frac{SSE}{SSTO}=\frac{SSR}{SSTO}$

- when comparing models the one with the larger $R^2$ is better

- models can be tuned to have the same $R^2$

![](img/img6.png)

- Model II and II should not be assumed linear models, but all three have the same $R^2$ 

- There is a better way to check from beginning if linear model is correct 

- Final model will be incorrect if initial assumption is incorrect 

- Wednesday we will cover a property of $R^2$ and start chapter 3

### Summary 

- C.I. of $y_\text{new}$

1. Only 1 $x_{\text{new}}$

- $\{\hat{y}_{\text{new}}\pm t_{}\cdot \sqrt{1+\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

2. m identical $x_{\text{new}}$

- $\{\hat{y}_{\text{new}}\pm t_{\frac{\alpha}{2},n-2}\cdot \sqrt{\frac{1}{m}+\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

3. Expected Prediction of $x_\text{new}$

- $\{\hat{y}_{\text{new}}\pm t_{\frac{\alpha}{2},n-2}\cdot \hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x_{\text{new}}-\overline{x})^2}{s_{xx}}}$

Coefficient of Determination : $R^2=1-\frac{SSE}{SSTO}=\frac{SSR}{SSTO}$

### To do 

- complete todays notes (complete)

- HW2 due Wednesday 

  - use t test (haven't covered f-test)
  
- read chapter 3 (begin on Wednesday)

- rewatch mon. Oct. 4th lecture (complete)

# W4-D11 : Wed. Oct. 20th, 2021


### Summary

### To do

- rewatch wed. Oct. 6th lecture

# W4-D12 : Fri. Oct. 22nd, 2021

### Summary

### To do

- rewatch fri. Oct. 8th lecture 
