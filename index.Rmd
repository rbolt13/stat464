---
title: "Notes"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output:
  html_document:
    toc: true
    toc_float: TRUE
    theme: spacelab
---

```{r,echo=F, message=F}
library(dplyr)
library(kableExtra)
```

# About

These are class notes and R code for Professor Ge Zhao's STAT-464 : Applied Regression Analysis for Fall term 2021 at Portland State University. 

# D1 : Mon. September 27, 2021

- will ask around for a bigger room 

- assume we have basic understanding of R 

## Introduction : Chapter 1 

- We want to know if the data matches (1-1), (1-Many), (Many - Many) and such 

- We call the data we are interested in the response $Y$ or sometimes the independet variable 

  - want to see what happens to y and the change of Y (predict Y)
  
  - examples : price of bitcoin, or price of house 

- We call the $X$ the predictor(s)

- want to use prediction to predict a response 

  - view of the model 
  
  - annalyze a model 
  
  - example (bitcoin) : day 1 = \$50, day 2 = \$100, day 3 = \$1,000
  
- math model is a statistical model, and to firgue out $Y=f(x)+\epsilon$ where $\epsilon$ is random error

  - in math model everything is fixed except for the $\epsilon$
  
  - this is a very general problem (tells almost nothing)
  
```{r, echo=F}
X <- c("continuous", "categrorical : ordinal data and nominal data")
Y <- c("continuous", "categorical : ordinal data and nominal data")

data.frame(X,Y) %>% kable() %>% kable_paper
```
- ordinal data (can be ordered) and nominal data (different types of data)"

- in regression class most of the time we are focuses on X=Y : Continuous vs Continuous 

- in experimental design we are focused on Y=X : Continuous vs Categorical 

- special case (in later chapters) Y=X : Ordinal vs. continuous 

- later (in other classes) we may see (double line) experimental design looks at every combination of categorical vs. categorical .

### Summary / Thoughts

We are focusing on data, and relations between $X$ and $Y$. 

Regressions would have continuous vs. continuous. This reminds me graphically of physics problems. How far does something go in a certain amount of time. 

Experimental design would have a continuous $Y$ vs. a Categorical $X$. Graphically I think of histograms. 

Some of the special cases I would need to think about. 

# D2 : Wed. September 29, 2021

- If you cannot attend in person use zoom number in D2L link 

### Simple Linear Regression Model 

A model with a single regressor (predictor) x that has a relationship with a response y that is a straight line line. $$y=\beta _{0}+\beta _{1}x+\epsilon$$

- $x$ is the data, $\beta _{0}$ is the intercept, $\beta _{1}$ is the slope, and $\epsilon$ is a random error component. 

  - $\beta$ unknown constants (parameters)
  
  - x is independent variable (predictor or regressor) and y is dependent variable (response).
  
  - simple because there is only one regressor (x).
  
  - the slope $\beta_{1}$ is the change in the mean of the distribution of y produced by a unit change in x.
  
  - example might be $x_1$ = pulse rate (rest rate), and $y$ might be oxygen consumption rate (oxygen rate)

- data points don't fall on a straight line so we can quantify the distances between each point and the best fit line

- $\epsilon$ is assumed to have an expectation or mean of 0 and an unknown variance $\sigma ^{2}$.

Why can you make the assumption that the expectation of $\epsilon$ is zero? 

- The expectation of a random variable must be a fixed random variable (number) whose derivative would be 0.

- can always absorb +c into $\beta$

- $\epsilon _{i}$ are uncorrelated from observation to observation 

### Mean and Variance of SLR

$$E(y|x)=\mu_{x|y}=E(\beta _{0}+\beta _{1}x+\epsilon)=\beta _{0}+\beta_{1}x$$

- $E(y|x)$ referes to the mean _y_ conditional on a specific value of _x_. 

$$Var(y|x)=\sigma^{2}_{y|x}=Var(\beta _{0}+\beta _{1}x+\epsilon)=\sigma^{2}$$

- homogenous variance assumption 

### Least-Square Estimation 

Suppose we have $n$ pairs of data : $(y_{1},x_{1}),(y_{2},x_{2}),...,(y_{n},x_{n})$. Then for a single point $$y_{i}=(\beta _{0}+\beta _{1}x+\epsilon_{i})$$

<center>

OR

</center>

$$\epsilon_{i}=y_{i}-\beta _{0}-\beta _{1}x$$

The idea is to minimize variations (the size of $\epsilon_{i}$), by taking the minimum : 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

$$S(\beta_{0},\beta{1})=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

- when the value is squared it will not be canceled with itself 

Optimize by taking the derivative, set that to zero, and solve

$\Rightarrow$ Take partial derivatives

$$\frac{\partial S}{\partial \beta_{0}}=\sum_{i=1}^{n}-2[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$$\frac{\partial S}{\partial \beta_{1}}=\sum_{i=1}^{n}-2x_{i}[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$\Rightarrow$ set both equations equal to 0

$$\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\beta _{0}-\sum_{i=1}^{n}\beta _{1}x_{i}=0$$

$$\sum_{i=1}^{n}y_{i}x_{i}-\sum_{i=1}^{n}\beta _{0}x_{i}-\sum_{i=1}^{n}\beta _{1}x_{i}^{2}=0$$

### Summary 

- Class notes are difficult to see and hear, but follow the online notes from another student from a past term. Will try to stay in front of reading both textbook and old notes before class period. Will need to spend some time everyday on this class. 

- Data = Model + Error , where error is a deviation from the model 

- Discussed Simple Linear Regression Model, Expectation (mean), and variance, as well as the least squares estimate

- $\beta_{0}$ is the intercept, and $\beta_{1}$ is the slope

# D3 : Fri. October 1st, 2021

### Properties of least squares estimates

Today we will prove ??? 


This is the SSR form 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$



$\overline{xy}$ is **not** $x*y$ but the area


- rewrite solutions :
  - cancle out n's
  - multiple $\overline{x}^{2}$  
  - cancle the terms with beta 0 
  
$\beta_{1}\{\overline{x}^2-\{\overline{x}\}^{2}\}=\overline{xy}-\overline{x} \cdot\overline{y}$

From (1) we can have relationship $\beta_{0}=\overline{y}-\beta_{1}\overline{x}$


...

12:03

$\hat{\beta}=\frac{\overline{xy}-\overline{x} \cdot\overline{y}}{\overline{x}^{2}-\{\overline{x}\}^{2}}=\frac{\sum}{\sum}$

### Summary

- Trying to follow along with proofs that I can't see isn't working. 

- If time this term, it wouldn't hurt to revist these lectures and try to fill out these notes. 

- [October 1](http://web.pdx.edu/~gibson25/564/564-lecture-notes.html#12_-_october_1,_2020) should be the proof we did today. 

# D4 : Mon. October 4th, 2021

- class is very difficult to follow along to, so these are more reflective of what's in the book and from notes of students from last year. 

- it seems like we work through proofs in class, but I do not think that is knowledge I am required to know to pass this class so ... oh well.  

- do not hand write homework 

- rmarkdown file  is hell, and will take me a couple hours to clean up before I can use it

- Final / Midterm will be in the form of an official report 

### Properties of least squares estimates

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of of the observations $y_{i}$

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$ 

- the expectation of $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbias estimators 

- this means estimators are good because if we repeat test many times, the model will show very close to the truth. 

### Proof that expectations of estimators equals parameters 


We need to prove that $E[\hat{\beta}_{0}]=\beta_{0}$ and $E[\hat{\beta}_{1}]=\beta_{1}$

First assume that $E[\hat{\beta}_{1}]=\beta_{1}$. 

\begin{equation}\label{estimators expectation proof}
\begin{split}
E[\hat{\beta}_{0}] & = E[\overline{Y}-\hat{\beta}_{1}\overline{x}]\\
& = E[\overline{Y}]-\overline{x}E[\hat{\beta}_{1}]\\
& = \frac{1}{n}\sum_{i=1}^{n}E[\overline{Y}]-\overline{x}\hat{\beta}_{1}\\
& = \frac{1}{n}\sum_{i=1}^{n}(\beta_{0} +\beta_{1}x_{i})-\overline{x}\hat{\beta}_{1}\\
& = \beta_{0} +\overline{x}\hat{\beta}_{1}-\overline{x}\hat{\beta}_{1}\\
& = \beta_0
\end{split}
\end{equation}

Therefore by the proof of equalities $E[\hat{\beta}_{0}]=\beta_{0}$. QED. 

- y : not random, Y : random 

- E(ax+bY)=aE(x)+bE(Y)

- cannot assume x to be random because it is apart of the observed data 


### Summary 

- cancellation is tricky, will revisit on Wednesday 

- nothing in the homework has really been covered in class. Not sure what that's about, and I don't care. Heavily considering ditching class notes just to follow along with the textbook. seems more applicable to what I will need to do / know for the test and future, as opposed to whatever it is the professor spends all of lecture doing (which I think is mostly just algebra?). 

# D5 : Wed. October 6th, 2021

- there is a 300 level version of this course  

- next we show we can compute the variance of $\hat{\beta}_1$ and $\hat{\beta}_0$. 

### Properties of least squares estimates

3. The variance of $\hat{\beta}_1$ and $\hat{\beta}_0$

**<span style="color: violet;">Assumptions</span>**

$E(\epsilon )=0$

$Var(\epsilon)=\sigma^2$

$\epsilon_i$ and $\epsilon_j$ are independent. ($\epsilon_i \ne \epsilon_j$)

$E(\epsilon_i\cdot\epsilon_j)=0$

$Cov(\epsilon_i\cdot\epsilon_j)=0$

Note : the only term with randomness is $\hat{y}$

### Variance of least squares estimates

\begin{equation}\label{var of slope estimate}
\begin{split}
Var[\hat{\beta}_{1}] & = Var\{\sum_{i=1}^{n}\frac{x_{i}-\overline{x}}{S_{xx}}Y_{i}\}^2\\
& = \sum_{i=1}^{n}Var\{\frac{x_{i}-\overline{x}}{S_{xx}}Y_{i}\}^2+0(\text{Cov.})\\
& = \sum_{i=1}^{n}\{\frac{x_{i}-\overline{x}}{S_{xx}}\}^{2}Var(Y_{i})^2\\
& = \sum_{i=1}^{n}\{\frac{x_{i}-\overline{x}}{S_{xx}}\}^{2}\sigma^2\\
& = \frac{\sigma^2}{S_{xx}^2}\sum_{i=1}^{n}(x_{i}-\overline{x})
\end{split}
\end{equation}

\begin{equation}\label{var of intercept estimate}
\begin{split}
Var[\hat{\beta}_{0}] & = Var(\overline{Y}-\hat{\beta}_1\overline{x})\\
& = Var\{\frac{1}{n}\sum_{i=1}^{n}Y_{i}-\overline{x}\sum_{i=1}^{n}\frac{x_{i}-\overline{x}}{S_{xx}}Y_i\}^2\\
& = Var[\sum_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}Y_i]^2\\
& = \sum_{i=1}^{n}Var\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2Y_i\\
& = \sum_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2\sigma^2
\end{split}
\end{equation}

- remember that $\beta_{0}$ is y intercept 

### Sum of residuals is zero

4. $e_{i}=y_i+\hat{y}_i$

> $\sum_{i=1}^{n}e_i=0$
> 
> $\sum_{i=1}^{n}u=\hat{y}_i$

- proof is simple (and will be added to homework) : just plug in $\hat{y}_i$ and things will cancel 

- make sure to show results of one to be true before using it to prove the other

### Least Squares goes through the centroid of the data

5. $\overline{y}=\hat{\beta}_0+\hat{\beta}_1\overline{x}$

- centroid is $(x_i,y_i)_{i=1}^n$

- proof is simple just plug in estimator 

### LS Weighted by the corresponding fitted value equals zero 

6. $\sum_{i=1}^{n}x_ie_i=0$ and $\sum_{i=1}^{n}\hat{y}_ie_i=0$

> $\sum_{i=1}^{n}x_ie_i=0$
>
> $\sum_{i=1}^{n}\hat{y}_ie_i=0$

- need to expand to prove

- will be additional homework problem 

- residual is particular to x

- residual is particular to $\hat{y}$

- $x_i$ and $\hat{y}_i$ are 1 dimennsional vectors 

### Is this the correct line? 

- Original problem : When we assume a linear model how do we know we are using the correct line? 

- Is our assumption correct? 

- Is the model useful? 

- We do not know the parameters ($beta_0$ and $\beta_1$) (intercept and slope) so we estimated them 

- The answer will be an inference, but before we can make that inference we neet to estimate $\sigma^2$

- In most real life cases we do not know $\sigma^2$

### Estimating sigma squared 


### Summary 

- There are 6 properties of our least squares estimators $\hat{\beta}_0$ and $\hat{\beta}_1$

1. 

2. 

3. 

4. 

5.

6.

- dont do one of the homework problems

# D6 : Fri. October 8th, 2021
