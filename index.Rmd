---
title: "Notes"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output:
  html_document:
    toc: true
    toc_float: TRUE
    theme: spacelab
---

```{r,echo=F, message=F}
library(dplyr)
library(kableExtra)
```

# About

These are class notes and R code for Professor Ge Zhao's STAT-464 : Applied Regression Analysis for Fall term 2021 at Portland State University. 

# D1 : Monday September 27, 2021

- will ask around for a bigger room 

- assume we have basic understanding of R 

## Introduction : Chapter 1 

- We want to know if the data matches (1-1), (1-Many), (Many - Many) and such 

- We call the data we are interested in the response $Y$ or sometimes the independet variable 

  - want to see what happens to y and the change of Y (predict Y)
  
  - examples : price of bitcoin, or price of house 

- We call the $X$ the predictor(s)

- want to use prediction to predict a response 

  - view of the model 
  
  - annalyze a model 
  
  - example (bitcoin) : day 1 = \$50, day 2 = \$100, day 3 = \$1,000
  
- math model is a statistical model, and to firgue out $Y=f(x)+\epsilon$ where $\epsilon$ is random error

  - in math model everything is fixed except for the $\epsilon$
  
  - this is a very general problem (tells almost nothing)
  
```{r, echo=F}
X <- c("continuous", "categrorical : ordinal data and nominal data")
Y <- c("continuous", "categorical : ordinal data and nominal data")

data.frame(X,Y) %>% kable() %>% kable_paper
```
- ordinal data (can be ordered) and nominal data (different types of data)"

- in regression class most of the time we are focuses on X=Y : Continuous vs Continuous 

- in experimental design we are focused on Y=X : Continuous vs Categorical 

- special case (in later chapters) Y=X : Ordinal vs. continuous 

- later (in other classes) we may see (double line) experimental design looks at every combination of categorical vs. categorical .

### Summary / Thoughts

We are focusing on data, and relations between $X$ and $Y$. 

Regressions would have continuous vs. continuous. This reminds me graphically of physics problems. How far does something go in a certain amount of time. 

Experimental design would have a continuous $Y$ vs. a Categorical $X$. Graphically I think of histograms. 

Some of the special cases I would need to think about. 

# D2 : Wednesday September 29, 2021

- If you cannot attend in person use zoom number in D2L link 

### Simple Linear Regression Model 

A model with a single regressor (predictor) x that has a relationship with a response y that is a straight line line. $$y=\beta _{0}+\beta _{1}x+\epsilon$$

- $x$ is the data, $\beta _{0}$ is the intercept, $\beta _{1}$ is the slope, and $\epsilon$ is a random error component. 

  - $\beta$ unknown constants (parameters)
  
  - x is independent variable (predictor or regressor) and y is dependent variable (response).
  
  - simple because there is only one regressor (x).
  
  - the slope $\beta_{1}$ is the change in the mean of the distribution of y produced by a unit change in x.
  
  - example might be $x_1$ = pulse rate (rest rate), and $y$ might be oxygen consumption rate (oxygen rate)

- data points don't fall on a straight line so we can quantify the distances between each point and the best fit line

- $\epsilon$ is assumed to have an expectation or mean of 0 and an unknown variance $\sigma ^{2}$.

Why can you make the assumption that the expectation of $\epsilon$ is zero? 

- The expectation of a random variable must be a fixed random variable (number) whose derivative would be 0.

- can always absorb +c into $\beta$

- $\epsilon _{i}$ are uncorrelated from observation to observation 

### Mean and Variance of SLR

$$E(y|x)=\mu_{x|y}=E(\beta _{0}+\beta _{1}x+\epsilon)=\beta _{0}+\beta_{1}x$$

- $E(y|x)$ referes to the mean _y_ conditional on a specific value of _x_. 

$$Var(y|x)=\sigma^{2}_{y|x}=Var(\beta _{0}+\beta _{1}x+\epsilon)=\sigma^{2}$$

- homogenous variance assumption 

### Least-Square Estimation 

Suppose we have $n$ pairs of data : $(y_{1},x_{1}),(y_{2},x_{2}),...,(y_{n},x_{n})$. Then for a single point $$y_{i}=(\beta _{0}+\beta _{1}x+\epsilon_{i})$$

<center>

OR

</center>

$$\epsilon_{i}=y_{i}-\beta _{0}-\beta _{1}x$$

The idea is to minimize variations (the size of $\epsilon_{i}$), by taking the minimum : 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

$$S(\beta_{0},\beta{1})=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

- when the value is squared it will not be canceled with itself 

Optimize by taking the derivative, set that to zero, and solve

$\Rightarrow$ Take partial derivatives

$$\frac{\partial S}{\partial \beta_{0}}=\sum_{i=1}^{n}-2[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$$\frac{\partial S}{\partial \beta_{1}}=\sum_{i=1}^{n}-2x_{i}[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$\Rightarrow$ set both equations equal to 0

$$\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\beta _{0}-\sum_{i=1}^{n}\beta _{1}x_{i}=0$$

$$\sum_{i=1}^{n}y_{i}x_{i}-\sum_{i=1}^{n}\beta _{0}x_{i}-\sum_{i=1}^{n}\beta _{1}x_{i}^{2}=0$$

### Summary 

- Class notes are difficult to see and hear, but follow the online notes from another student from a past term. Will try to stay in front of reading both textbook and old notes before class period. Will need to spend some time everyday on this class. 

- Data = Model + Error , where error is a deviation from the model 

- Discussed Simple Linear Regression Model, Expectation (mean), and variance, as well as the least squares estimate

- $\beta_{0}$ is the intercept, and $\beta_{1}$ is the slope

# D3 : Friday October 1st, 2021

### Properties of least squares estimates

Today we will prove ??? 


This is the SSR form 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$



$\overline{xy}$ is **not** $x*y$ but the area


- rewrite solutions :
  - cancle out n's
  - multiple $\overline{x}^{2}$  
  - cancle the terms with beta 0 
  
$\beta_{1}\{\overline{x}^2-\{\overline{x}\}^{2}\}=\overline{xy}-\overline{x} \cdot\overline{y}$

From (1) we can have relationship $\beta_{0}=\overline{y}-\beta_{1}\overline{x}$


...

12:03

$\hat{\beta}=\frac{\overline{xy}-\overline{x} \cdot\overline{y}}{\overline{x}^{2}-\{\overline{x}\}^{2}}=\frac{\sum}{\sum}$

### Summary

- Trying to follow along with proofs that I can't see isn't working. 

- If time this term, it wouldn't hurt to revist these lectures and try to fill out these notes. 

- [October 1](http://web.pdx.edu/~gibson25/564/564-lecture-notes.html#12_-_october_1,_2020) should be the proof we did today. 

# D4 : Monday October 4th, 2021

- class is very difficult to follow along to, so these are more reflective of what's in the book and from notes of students from last year. 

- it seems like we work through proofs in class, but I do not think that is knowledge I am required to know to pass this class so ... oh well. Typical PSU. 

- do not hand write homework 

- rmarkdown file  is hell, and will take me a couple hours to clean up before I can use it

- Final / Midterm will be in the form of an official report 

### Properties of least squares estimates

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of of the observations $y_{i}$

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$ 

- the expectation of $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbias estimators 

- this means estimators are good because if we repeat test many times, the model will show very close to the truth. 

### Proof that expectations of estimators equals parameters 


We need to prove that $E[\hat{\beta}_{0}]=\beta_{0}$ and $E[\hat{\beta}_{1}]=\beta_{1}$

First assume that $E[\hat{\beta}_{1}]=\beta_{1}$. 

\begin{equation}\label{estimators expectation proof}
\begin{split}
E[\hat{\beta}_{0}] & = E[\overline{Y}-\hat{\beta}_{1}\overline{x}]\\
& = E[\overline{Y}]-\overline{x}E[\hat{\beta}_{1}]\\
& = \frac{1}{n}\sum_{i=1}^{n}E[\overline{Y}]-\overline{x}\hat{\beta}_{1}\\
& = \frac{1}{n}\sum_{i=1}^{n}(\beta_{0} +\beta_{1}x_{i})-\overline{x}\hat{\beta}_{1}\\
& = \beta_{0} +\overline{x}\hat{\beta}_{1}-\overline{x}\hat{\beta}_{1}\\
& = \beta_0
\end{split}
\end{equation}

Therefore by the proof of equalities $E[\hat{\beta}_{0}]=\beta_{0}$. QED. 

- y : not random, Y : random 

- E(ax+bY)=aE(x)+bE(Y)

- cannot assume x to be random because it is apart of the observed data 


### Summary 

- cancellation is tricky, will revisit on Wednesday 

- nothing in the homework has really been covered in class. Not sure what that's about, and I don't care. Heavily considering ditching class notes just to follow along with the textbook. seems more applicable to what I will need to do / know for the test and future, as opposed to whatever it is the professor spends all of lecture doing (which is think is mostly just algebra?). 

# D5 : Wednesday October 6th, 2021




