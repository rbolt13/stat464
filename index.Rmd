---
title: "Notes"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output:
  html_document:
    toc: true
    toc_float: TRUE
    theme: spacelab
---

```{r,echo=F, message=F}
library(dplyr)
library(kableExtra)
```

# About

These are class notes and R code for Professor Ge Zhao's STAT-464 : Applied Regression Analysis for Fall term 2021 at Portland State University. 

# D1 : Mon. Sept. 27, 2021

- will ask around for a bigger room 

- assume we have basic understanding of R 

## Introduction : Chapter 1 

- We want to know if the data matches (1-1), (1-Many), (Many - Many) and such 

- We call the data we are interested in the response $Y$ or sometimes the independet variable 

  - want to see what happens to y and the change of Y (predict Y)
  
  - examples : price of bitcoin, or price of house 

- We call the $X$ the predictor(s)

- want to use prediction to predict a response 

  - view of the model 
  
  - annalyze a model 
  
  - example (bitcoin) : day 1 = \$50, day 2 = \$100, day 3 = \$1,000
  
- math model is a statistical model, and to firgue out $Y=f(x)+\epsilon$ where $\epsilon$ is random error

  - in math model everything is fixed except for the $\epsilon$
  
  - this is a very general problem (tells almost nothing)
  
```{r, echo=F}
X <- c("continuous", "categrorical : ordinal data and nominal data")
Y <- c("continuous", "categorical : ordinal data and nominal data")

data.frame(X,Y) %>% kable() %>% kable_paper
```
- ordinal data (can be ordered) and nominal data (different types of data)"

- in regression class most of the time we are focuses on X=Y : Continuous vs Continuous 

- in experimental design we are focused on Y=X : Continuous vs Categorical 

- special case (in later chapters) Y=X : Ordinal vs. continuous 

- later (in other classes) we may see (double line) experimental design looks at every combination of categorical vs. categorical .

### Summary / Thoughts

We are focusing on data, and relations between $X$ and $Y$. 

Regressions would have continuous vs. continuous. This reminds me graphically of physics problems. How far does something go in a certain amount of time. 

Experimental design would have a continuous $Y$ vs. a Categorical $X$. Graphically I think of histograms. 

Some of the special cases I would need to think about. 

# D2 : Wed. Sept. 29, 2021

- If you cannot attend in person use zoom number in D2L link 

### Simple Linear Regression Model 

A model with a single regressor (predictor) x that has a relationship with a response y that is a straight line line. $$y=\beta _{0}+\beta _{1}x+\epsilon$$

- $x$ is the data, $\beta _{0}$ is the intercept, $\beta _{1}$ is the slope, and $\epsilon$ is a random error component. 

  - $\beta$ unknown constants (parameters)
  
  - x is independent variable (predictor or regressor) and y is dependent variable (response).
  
  - simple because there is only one regressor (x).
  
  - the slope $\beta_{1}$ is the change in the mean of the distribution of y produced by a unit change in x.
  
  - example might be $x_1$ = pulse rate (rest rate), and $y$ might be oxygen consumption rate (oxygen rate)

- data points don't fall on a straight line so we can quantify the distances between each point and the best fit line

- $\epsilon$ is assumed to have an expectation or mean of 0 and an unknown variance $\sigma ^{2}$.

Why can you make the assumption that the expectation of $\epsilon$ is zero? 

- The expectation of a random variable must be a fixed random variable (number) whose derivative would be 0.

- can always absorb +c into $\beta$

- $\epsilon _{i}$ are uncorrelated from observation to observation 

### Mean and Variance of SLR

$$E(y|x)=\mu_{x|y}=E(\beta _{0}+\beta _{1}x+\epsilon)=\beta _{0}+\beta_{1}x$$

- $E(y|x)$ referes to the mean _y_ conditional on a specific value of _x_. 

$$Var(y|x)=\sigma^{2}_{y|x}=Var(\beta _{0}+\beta _{1}x+\epsilon)=\sigma^{2}$$

- homogenous variance assumption 

### Least-Square Estimation 

Suppose we have $n$ pairs of data : $(y_{1},x_{1}),(y_{2},x_{2}),...,(y_{n},x_{n})$. Then for a single point $$y_{i}=(\beta _{0}+\beta _{1}x+\epsilon_{i})$$

<center>

OR

</center>

$$\epsilon_{i}=y_{i}-\beta _{0}-\beta _{1}x$$

The idea is to minimize variations (the size of $\epsilon_{i}$), by taking the minimum : 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

$$S(\beta_{0},\beta{1})=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$

- when the value is squared it will not be canceled with itself 

Optimize by taking the derivative, set that to zero, and solve

$\Rightarrow$ Take partial derivatives

$$\frac{\partial S}{\partial \beta_{0}}=\sum_{i=1}^{n}-2[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$$\frac{\partial S}{\partial \beta_{1}}=\sum_{i=1}^{n}-2x_{i}[y_{i}-\beta _{0}-\beta _{1}x_{i}]$$

$\Rightarrow$ set both equations equal to 0

$$\sum_{i=1}^{n}y_{i}-\sum_{i=1}^{n}\beta _{0}-\sum_{i=1}^{n}\beta _{1}x_{i}=0$$

$$\sum_{i=1}^{n}y_{i}x_{i}-\sum_{i=1}^{n}\beta _{0}x_{i}-\sum_{i=1}^{n}\beta _{1}x_{i}^{2}=0$$

### Summary 

- Class notes are difficult to see and hear, but follow the online notes from another student from a past term. Will try to stay in front of reading both textbook and old notes before class period. Will need to spend some time everyday on this class. 

- Data = Model + Error , where error is a deviation from the model 

- Discussed Simple Linear Regression Model, Expectation (mean), and variance, as well as the least squares estimate

- $\beta_{0}$ is the intercept, and $\beta_{1}$ is the slope

# D3 : Fri. Oct. 1st, 2021

### Properties of least squares estimates

Today we will prove ??? 


This is the SSR form 

$$\min\limits_{\beta_{0},\beta{1}}=\sum_{i=1}^n[y_{i}-\beta _{0}-\beta _{1}x_{i}]^{2}$$



$\overline{xy}$ is **not** $x*y$ but the area


- rewrite solutions :
  - cancle out n's
  - multiple $\overline{x}^{2}$  
  - cancle the terms with beta 0 
  
$\beta_{1}\{\overline{x}^2-\{\overline{x}\}^{2}\}=\overline{xy}-\overline{x} \cdot\overline{y}$

From (1) we can have relationship $\beta_{0}=\overline{y}-\beta_{1}\overline{x}$


...

12:03

$\hat{\beta}=\frac{\overline{xy}-\overline{x} \cdot\overline{y}}{\overline{x}^{2}-\{\overline{x}\}^{2}}=\frac{\sum}{\sum}$

### Summary

- Trying to follow along with proofs that I can't see isn't working. 

- If time this term, it wouldn't hurt to revist these lectures and try to fill out these notes. 

- [October 1](http://web.pdx.edu/~gibson25/564/564-lecture-notes.html#12_-_october_1,_2020) should be the proof we did today. 

# D4 : Mon. Oct. 4th, 2021

- class is very difficult to follow along to, so these are more reflective of what's in the book and from notes of students from last year. 

- it seems like we work through proofs in class, but I do not think that is knowledge I am required to know to pass this class so ... oh well.  

- do not hand write homework 

- rmarkdown file  is hell, and will take me a couple hours to clean up before I can use it

- Final / Midterm will be in the form of an official report 

### Properties of least squares estimates

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of of the observations $y_{i}$

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$ 

- the expectation of $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbias estimators 

- this means estimators are good because if we repeat test many times, the model will show very close to the truth. 

### Proof that expectations of estimators equals parameters 


We need to prove that $E[\hat{\beta}_{0}]=\beta_{0}$ and $E[\hat{\beta}_{1}]=\beta_{1}$

First assume that $E[\hat{\beta}_{1}]=\beta_{1}$. 

\begin{equation}\label{estimators expectation proof}
\begin{split}
E[\hat{\beta}_{0}] & = E[\overline{Y}-\hat{\beta}_{1}\overline{x}]\\
& = E[\overline{Y}]-\overline{x}E[\hat{\beta}_{1}]\\
& = \frac{1}{n}\sum_{i=1}^{n}E[\overline{Y}]-\overline{x}\hat{\beta}_{1}\\
& = \frac{1}{n}\sum_{i=1}^{n}(\beta_{0} +\beta_{1}x_{i})-\overline{x}\hat{\beta}_{1}\\
& = \beta_{0} +\overline{x}\hat{\beta}_{1}-\overline{x}\hat{\beta}_{1}\\
& = \beta_0
\end{split}
\end{equation}

Therefore by the proof of equalities $E[\hat{\beta}_{0}]=\beta_{0}$. QED. 

- y : not random, Y : random 

- E(ax+bY)=aE(x)+bE(Y)

- cannot assume x to be random because it is apart of the observed data 


### Summary 

- cancellation is tricky, will revisit on Wednesday 

- nothing in the homework has really been covered in class. Not sure what that's about, and I don't care. Heavily considering ditching class notes just to follow along with the textbook. seems more applicable to what I will need to do / know for the test and future, as opposed to whatever it is the professor spends all of lecture doing (which I think is mostly just algebra?). 

# D5 : Wed. Oct. 6th, 2021

- there is a 300 level version of this course  

- next we show we can compute the variance of $\hat{\beta}_1$ and $\hat{\beta}_0$. 

### Properties of least squares estimates

3. The variance of $\hat{\beta}_1$ and $\hat{\beta}_0$

**<span style="color: violet;">Assumptions</span>**

$E(\epsilon )=0$

$Var(\epsilon)=\sigma^2$

$\epsilon_i$ and $\epsilon_j$ are independent. ($\epsilon_i \ne \epsilon_j$)

$E(\epsilon_i\cdot\epsilon_j)=0$

$Cov(\epsilon_i\cdot\epsilon_j)=0$

Note : the only term with randomness is $\hat{y}$

### Variance of least squares estimates

\begin{equation}\label{var of slope estimate}
\begin{split}
Var[\hat{\beta}_{1}] & = Var\{\sum_{i=1}^{n}\frac{x_{i}-\overline{x}}{S_{xx}}Y_{i}\}^2\\
& = \sum_{i=1}^{n}Var\{\frac{x_{i}-\overline{x}}{S_{xx}}Y_{i}\}^2+0(\text{Cov.})\\
& = \sum_{i=1}^{n}\{\frac{x_{i}-\overline{x}}{S_{xx}}\}^{2}Var(Y_{i})^2\\
& = \sum_{i=1}^{n}\{\frac{x_{i}-\overline{x}}{S_{xx}}\}^{2}\sigma^2\\
& = \frac{\sigma^2}{S_{xx}^2}\sum_{i=1}^{n}(x_{i}-\overline{x})
\end{split}
\end{equation}

\begin{equation}\label{var of intercept estimate}
\begin{split}
Var[\hat{\beta}_{0}] & = Var(\overline{Y}-\hat{\beta}_1\overline{x})\\
& = Var\{\frac{1}{n}\sum_{i=1}^{n}Y_{i}-\overline{x}\sum_{i=1}^{n}\frac{x_{i}-\overline{x}}{S_{xx}}Y_i\}^2\\
& = Var[\sum_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}Y_i]^2\\
& = \sum_{i=1}^{n}Var\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2Y_i\\
& = \sum_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2\sigma^2
\end{split}
\end{equation}

- remember that $\beta_{0}$ is y intercept 

### Sum of residuals is zero

4. $e_{i}=y_i+\hat{y}_i$

> $\sum_{i=1}^{n}e_i=0$
> 
> $\sum_{i=1}^{n}u=\hat{y}_i$

- proof is simple (and will be added to homework) : just plug in $\hat{y}_i$ and things will cancel 

- make sure to show results of one to be true before using it to prove the other

### Least Squares goes through the centroid of the data

5. $\overline{y}=\hat{\beta}_0+\hat{\beta}_1\overline{x}$

- centroid is $(x_i,y_i)_{i=1}^n$

- proof is simple just plug in estimator 

### LS Weighted by the corresponding fitted value equals zero 

6. $\sum_{i=1}^{n}x_ie_i=0$ and $\sum_{i=1}^{n}\hat{y}_ie_i=0$

> $\sum_{i=1}^{n}x_ie_i=0$
>
> $\sum_{i=1}^{n}\hat{y}_ie_i=0$

- need to expand to prove

- will be additional homework problem 

- residual is particular to x

- residual is particular to $\hat{y}$

- $x_i$ and $\hat{y}_i$ are 1 dimennsional vectors 

### Is this the correct line? 

- Original problem : When we assume a linear model how do we know we are using the correct line? 

- Is our assumption correct? 

- Is the model useful? 

- We do not know the parameters ($beta_0$ and $\beta_1$) (intercept and slope) so we estimated them 

- The answer will be an inference, but before we can make that inference we neet to estimate $\sigma^2$

- In most real life cases we do not know $\sigma^2$

### Estimating sigma squared 

Recall $e=y-\hat{y}$ 

$\sigma^2=Var(\epsilon)$

$\epsilon = Y -(\beta_0+\beta_1x)$

$e=y-(\hat{\beta}_0+\hat{\beta}_1x)=y-\hat{y}$

- idea is that we use the residuals to estimate the variance of error

Lets define these two quantities : 

$SS_{Res}=\sum\limits_{i=1}^{n}e^2_i=\sum\limits_{i=1}^{n}(y-\hat{y})^2$

$E(\sum\limits_{i=1}^{n}e^2)=(n-2)\sigma^2$

$\Rightarrow \sigma^2=\frac{E(\sum\limits_{i=1}^{n}e^2)}{n-2}$

- we don't know the true sigma, so we use this quantity to estimate sigma squared 

$\hat{\sigma}^2=\frac{\sum\limits_{i=1}^{n}e^2}{n-2}$

- because if we take the expectation of both sides we will have sigma squared 

$E(\hat{\sigma}^2)=E(\frac{\sum\limits_{i=1}^{n}e^2}{n-2})=\sigma^2$

- will be used a lot in the future

- note that this estimate is model dependent 

- in textbook as $MS_{\text{Res}}$ and sometimes called residual mean square

- square root of sigma squared sometimes called standard error of regression

### Summary 

- homework due monday 

- skip problems that deal with confidence interval 

- will go over some of the coding in class

- There are 6 properties of our least squares estimators $\hat{\beta}_0$ and $\hat{\beta}_1$

1. $\hat{\beta}_1$ and $\hat{\beta}_0$ are linear combinations of of the observations $y_{i}$

2. $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased estimators of $\beta_1$ and $\beta_0$ 

3. Variance of LS estimates $\hat{\beta}_0$ and $\hat{\beta}_1$

- $Var[\hat{\beta}_{1}]=\frac{\sigma^2}{S_{xx}^2}\sum\limits_{i=1}^{n}(x_{i}-\overline{x})$

- $Var[\hat{\beta}_{0}]=\sum\limits_{i=1}^{n}\{\frac{1}{n}-\overline{x}\frac{x_{i}-\overline{x}}{S_{xx}}\}^2\sigma^2$

4. Sum of residuals is zero

- $e_{i}=y_i+\hat{y}_i$

- $\sum\limits_{i=1}^{n}e_i=0$
 
- $\sum\limits_{i=1}^{n}u=\hat{y}_i$

5. LS goes through the centroid of the data

- $\overline{y}=\hat{\beta}_0+\hat{\beta}_1\overline{x}$

6. LS, Weighted by the corresponding fitted value, equals zero 

- $\sum\limits_{i=1}^{n}x_ie_i=0$ and $\sum_{i=1}^{n}\hat{y}_ie_i=0$

Estimate sigma^2 : $\hat{\sigma}^2=\frac{\sum\limits_{i=1}^{n}e^2}{n-2}$

# D6 : Fri. Oct. 8th, 2021

- will go over coding in R 

- update R studio and packages 

- plan B : just use code in R (instead of Rmd) ... ?

- will upload html to D2L

- everything covered in class is covered in code (gray boxes)

- want to double check result with correct answer look for white boxes

- no actual help on homework, but we will see what happens I guess

### Hypothesis Testing 

- $\hat{y}=\hat{\beta}_0-\hat{\beta}_1\overline{x}$ is the estimated Y with estimated parameters 

- Why wouldn't our professor believe this : $Y=\beta_0+\beta_1x+\epsilon$, but would beleive this : $Y=\epsilon$?

- Recall that E($\epsilon$)=0 and VAR($\epsilon$)=$\sigma^2$ are unknown. 

- How do we know which Y is correct? 

- What is the difference between $\beta_1=0$ and $\beta_0=0$ OR $\beta_1\ne0$ and $\beta_0\ne0$

- If $\beta_1$ is zero then x shouldn't be included in the model. 

- $H_0=\beta_1=0$, $\beta_0=0$

- $H_0=\beta_1\ne0$, $\beta_0\ne0$

Question 1 : Do we really need x to explain y? 

- in math language : $\beta_1=0$

... to be continued next class. 

### Summary 

- finish homework (complete)

# D7 : Mon. Oct. 11th, 2021

- 2 proofs are due for homework 2 

### Section 2.3 Hypothesis testing on parameters 

- $H_0$ : $\beta_1=0$ and $H_1$ : $\beta_1\ne0$

- we want to test if a statement is true or not 

- if $H_0$ is true then our model : $Y=\beta_0+\beta_1x+\epsilon$ has nothing to do with x. 

  - meaning that Y is just a constant plus $\epsilon$
  
  - can't get any information from model 
  
  - this is a very strong statement 
  
  - and our testing problem 
  
### t-statistic

- Recall $\epsilon_i\sim N(0,1)$ and that $\epsilon_i$ and $\epsilon_j$ are independent with $i\ne j$

- If we have the assumption that $\epsilon_i$ is independent and identically distributed, then 

$$t=\frac{\hat{\beta}_1-\hat{\beta}_{10}}{se(\hat{\beta}_{10})\}}\sim t_{n-1}$$
  
- this ratio is the t statistic exzactly

- n-2 because there are two parameters 

- later there will 3 up to p parameters

- Proof for this depends on $\chi^2$ (chi squared distribution) 

- se is squared root varience 

- Initially we assume that $H_0$ is true ($\beta_1=0$), but if z value is large then $H_1$ may be false

- $t=\frac{\hat{\beta}_1-\hat{\beta}_{10}}{se(\hat{\beta}_{10})}$ is called the "tester statistic" 

  - if $H_0$ is true then t is close to 0
  
  - if $H_0$ is false then t is further away (closer to 1)
  
![](img/img1.webp)

- img. above is pdf of t-distribution 

- p-value = 2(area1) = 2(area2) = $2Pr(T>t)=Pr(T>|t|)$

- p value is the probability (between 0 and 1) that $H_0$ is true.

- a p-value that is < significance level (usually 0.05), the we reject $H_0$. 

  - (accepting $H_1$)
  
  - example : P value of 0.00000000005 we can reject 
  
- a p-value that is > significance level (usually 0.05), then we fail to reject $H_0$.

  - not deterministic result 
  
  - note p-value is only the probability
  
  - if we accept $H_1$ then we say it is true, which is not the case
  
  - example : P value of 0.25 we fail to reject 
  
- consider p-value as uncertainty 
  
### Special Case (sigma squared is known)

- $\sigma^2$ is known (happens in some homework problems)

- "ideal case" 

- $t=\frac{\hat{\beta}_1-\hat{\beta}_{10}}{se(\hat{\beta}_{10})\}}\sim N(0,1)$

### One sided Test 

- A two sided test is what we were looking at before. 

- Often Written : $H_0$ : $\beta_0=\beta_{10}$ vs. $H_1$ : $\beta_0\ne\beta_{10}$

- A one sided test is : $H_0$ : $\beta_0=\beta_{10}$ vs. $H_1$ : $\beta_0>\beta_{10}$

- if test is one sided, then it will depend which side the p-value is on 

- only one side is being counted ($\frac{1}{2}$ Area)

### Summary 

- will revisit t (and maybe prove in a different form)

- update code page

- create homework page? 

- fill in holes in notes 

- finish chapter 2 

- peek at chapter 3

# D8 : Wed. Oct. 13th, 2021

- this next section follows the book (something to do with $R^2$)

### Analysis of Variance 

$\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$

- SSTO = Sum Square Total

- when we calculate simple variance of $y_i$ we use this formula 

- sometimes we might have a $\frac{1}{n}$ or some other constant in front of the sigma, but we can forget about that 

- the data point ($y_i$) minus the center point ($\overline{y}$)

  - the distance between the point and data mean (visualize graphically)
  
- we will decompose this into two parts 

$\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{n}(\hat{y}_i-\overline{y})^2$

- the first part is the residual : $\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2$

  - SSE : Sum Square of Error
  
  - if our model perfectly fits the data then $\hat{y}$ will be $y_i$ and residual will be zero
  
  - $SS_{RE}$

- the second part is variability of fitted data (SSR): $\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$

  - $\overline{\hat{y}}=\overline{y}$ : average of estimated y is the same as the original value 
  
  - SSR : Sum Square of Regression 
  
  - $SS_{Reg}$
  
- everything is decomposing this : $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$ variance
  
  - this means that the variance of the data ($\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$) can be explained by the variance of the error ($\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2$) and the variance of model ($\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$)
  
  ### Proof : Analysis of Variance 
$\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$ is true. 

\begin{equation}\label{analysis of var. proof part a}
\begin{split}
\sum\limits_{i=1}^{n}(y_i-\overline{y})^2 & = \sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2\\
& = \sum\limits_{i=1}^{n}(y_i-\hat{y}_i+\hat{y}_i-\overline{y})^2\\
& = \sum\limits_{i=1}^{n}\{(y_i-\hat{y}_i)^2+(\hat{y}_i-\overline{y})^2+2(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})\}
\end{split}
\end{equation}

What we need to show is that this term : $(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})$ , is zero. 

- $\sum\limits_{i=1}^n(y_i-\hat{y}_i)(\hat{y}_i-\overline{y}) = 0$

\begin{equation}\label{aov lemma}
\begin{split}
\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)(\hat{y}_i-\overline{y}) & = \sum\limits_{i=1}^n(y_i-\hat{y}_i)\hat{y}_i-\sum\limits_{i=1}^n(y_i-\hat{y}_i)\overline{y}\\
& = \sum\limits_{i=1}^ne_i\hat{y}_i-\overline{y}\sum\limits_{i=1}^ne_i\\
& = 0 - 0\\
& = 0
\end{split}
\end{equation}

- note to show that $\sum\limits_{i=1}^ne_i\hat{y}_i=0$ write out each of the individual terms for $\hat{y}_i$ and $e_i$

Therefore $\sum\limits_{i=1}^n(y_i-\hat{y}_i)(\hat{y}_i-\overline{y}) = 0$ , and such $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{2}(\hat{y}_i-\overline{y})^2$

### Degrees of Freedom 

- in probability theory we need to look at the degrees of freedom : df

  - for SSTO :  df = n - 1
  
  - for SSE : df = n-2
  
    - $\hat{\beta}_0$ and $\hat{\beta}_1$

  - for SSR : df = 1

### Decomposing terms : SSE , SSR , MSE 

$1=\frac{SSE+SSR}{SSTO}=\frac{SSE}{SSTO}+\frac{SSR}{SSTO}$

- SSTO is fixed 

- $+\frac{SSR}{SSTO}$ is $R^2$ (we will talk more about $R^2$ when we talk about the confidence interval)

- $\frac{SSE}{SSTO}$ is 

- we use SSE to estimate the variance

  - once we have variance we can show probability, density, or distribustion of almost anything 

$SSE=\frac{n-2}{\sigma^2}\sim x^2_{n-2}$

  - chi squared n - 2 distribution 
  
  - $\frac{SSE}{\sigma^2}=MSE\frac{n-2}{\sigma^2}$

$SSR=1\cdot MSR\sim x_1^2$

  - chi squared 1 distribution 

  - $\frac{SSR}{\sigma^2}=1\cdot \frac{MSR}{\sigma^2}$

$E(MSE)=\sigma^2$

> $MSE=\frac{1}{n-2}\sum\limits_{i=1}^n(y_i-\hat{y}_i)^2$

$MSE=\frac{SSE}{n-2}$

$MSE=\frac{SSE}{\text{df of SSE}}$

$MSR=\frac{SSR}{\text{df of SSR}}$

### F and  T Statistic 

What happens when we take one and divide it by another one? 

- put the smaller one (degrees of freedom) ontop of the larger one

$\sqrt{\frac{MSR}{MSE}}=\frac{1\cdot\frac{MSR}{\sigma^2}}{MSE\frac{n-2}{\sigma^2}}=\frac{MSR}{(???)MSE}=\frac{\chi_1^2/1}{\chi_{n-2}^2/n-2}$

F statistic : $F_{1, n-2}\sim \frac{\chi_1^2/1}{\chi_{n-2}^2/n-2}$

$\frac{\chi^2_m/m}{\chi_n/n}\sim F_{m,n}$ , if $\chi^2_n$ is  (independent) from $\chi^2_m$ then 

  - $\newcommand{\indep}{\perp \!\!\! \perp}\chi^2_n\indep \chi^2_m$
  
We have another problem ... what about T distribution? 

$T\sim df_n$

What is $T^2$ distribution? 

- $T^2\sim F_n$

We can change these : $T\sim df_{n-2}$ and $T^2\sim F_{n-2}$

### Key Points

- The analysis equations is the most critical result 

- we also decompose this equation into many terms, with each term responding to error , regresseion with ... a bunch of stuff. 

- if we find that any term is too small to consider we can remove it (here is only two parts so not very complex)

Test : $\beta_1=0$ vs. $\beta_1\ne0$

### Confidence Interval of parameter

Pr($1,\beta_1<\mu=1-\alpha)=<\alpha\ne1$

- of course we want this probability to be 1

- currently we dont know if this is the number we have or not 

- next we are going to try to connect bact to $1-\alpha$

- probability of random vairable : $\beta_1$ is computed here

$t=\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}\sim t_{n-2}$

- distribution of $\beta_1$ is defined here 

Can we solve for our $\mu$? 

- if we can we can find the interval for $\beta_1$

- we will do this this Friday. 

### Summary 

- analysis of variance : $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2=\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum\limits_{i=1}^{n}(\hat{y}_i-\overline{y})^2$

- SSTO : sum of squares total : $\sum\limits_{i=1}^{n}(y_i-\overline{y})^2$

- SSE : sum of square error : $\sum\limits_{i=1}^{n}(y_i-\hat{y}_i)^2$

- SSR : sum of square regression : $\sum\limits_{i=1}^{n}(\hat{y}_i-\overline{y})^2$

- MSE : mean square error (??) : $MSE=\frac{1}{n-2}\sum\limits_{i=1}^n(y_i-\hat{y}_i)^2$

- F Statistic : $\frac{\chi^2_m/m}{\chi_n/n}\sim F_{m,n}$

- T Statistic : $T^2\sim F_{n-2}$

- Confidence interval for $\beta_1$'s 

  - probaility : Pr($1,\beta_1<\mu=1-\alpha)=<\alpha\ne1$
  
  - distribution : $t=\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}\sim t_{n-2}$
  
- HW 2 is live : Proofs and fill in skipped questions from HW 1

# D9 : Fri. Oct. 15th, 2021

- Problem is that we want to find the confidence interval (CI) for parameters $\beta_0$ and $\beta_1$, and $\sigma^2$

### CI For Beta_0 , Beta_1, and Sigma Squared

$Pr\{l<\beta_1<u\}=1-\alpha$

- we want probability that $\beta_1$ is greater than some lower bound and less than some upper bound is equal to some number, lets call alpha for now

- currently we don't know the lower and the upper 

  - we do know $t=\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}\sim t_{n-2}$ 
  
  - which is a different form of $\beta_1$
  
  - call this ratio t

- we also don't know the distribution of $\beta_1$. If we did we knew that we would know the probability and be able to find the lower and upper bounds. 

\begin{equation}\label{pr beta 1}
\begin{split}
1-\alpha & = Pr\{l<\beta_1<u\}\\
& = Pr\{-u<-\beta_1<-l\}\\
& = Pr\{\hat{\beta}_1-u<\hat{\beta}_1-\beta_1<\hat{\beta}_1-l\}\\
& = Pr\{\frac{\hat{\beta}_1-u}{s.e.(\hat{\beta}_1)}<\frac{\hat{\beta}_1-\beta_1}{s.e.(\hat{\beta}_1)}<\frac{\hat{\beta}_1-l}{s.e.(\hat{\beta}_1)}\}
\end{split}
\end{equation}

- now the probabilty becomes : 

$Pr(\frac{\hat{\beta}_1-u}{s.e.(\hat{\beta}_1)}<t<\frac{\hat{\beta}_1-l}{s.e.(\hat{\beta}_1)})=1-\alpha$

- we know t-distribution, so if we can figure our $\hat{\beta}_1$ and $s.e.(\hat{\beta}_1)$ then we can find upper and lower bounds 

![](img/img2.png)

- we have and even distribution with evenly distanced intevals a and -a 

- we know the area between these two points is $1-\alpha$

- that means the area of ends is $\frac{\alpha}{2}$

- $a=t_{\frac{\alpha}{2},n-2}$ , $t_{1-\frac{\alpha}{2},n-2}$ , $t_{n-2}^{\frac{\alpha}{2}}$

- plug in $a=t_{\frac{\alpha}{2},n-2}$ , then we have 

$\frac{\hat{\beta}_1-u}{s.e.(\hat{\beta}_1)}=-t_{\frac{\alpha}{2},n-2}$

$\Rightarrow u=\hat{\beta}_1+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

$\Rightarrow l=\hat{\beta}_1-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

- put this back in the original equation : $Pr\{l<\beta_1<u\}=1-\alpha$

- If we want a 99% confidence interval then our alpha needs to be 1%

- 95% CI, $\alpha=0.05$

For $Pr\{l<\beta_0<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\hat{\beta}_0+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

$\Rightarrow l=\hat{\beta}_0-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

### CI of Sigma Squared 

$\sigma^2$ ? 

$Pr\{l<\sigma^2<u\}=1-\alpha$

$\frac{(n-2)MSE}{\sigma^2}\sim \chi^2_{n-2}$

- this ratio is chi squared distribution with degrees of freedom n-2. wont prove but will need to know decompositions to prove

To help remember : 

$E(SSE)=(n-2)\sigma^2$

$MSE=\frac{1}{n-2}SSE$

![](img/img3.png)

- we are still looking for some interval c and d such that the blue area is $1-\alpha$. 

- area of a and b are : $a=\chi^2_{\frac{\alpha}{2},n-2}$ and $b=\chi^2_{1-\frac{\alpha}{2},n-2}$

$Pr\{\frac{(n-2)MSE}{u}<\frac{(n-2)MSE}{\sigma^2}<\frac{(n-2)MSE}{l}\}=1-\alpha$

- the lower probability is a and the upper probability is b so, 

$\Rightarrow u=\frac{(n-2)MSE}{\chi^2_{\frac{\alpha}{2},n-2}}$

$\Rightarrow l=\frac{(n-2)MSE}{\chi^2_{1-\frac{\alpha}{2},n-2}}$

- in this course we do not care too much for the CI of $\sigma^2$, but about the CI for $\beta_1$ and $\beta_0$

### Prediction 

- what people really want is the response, the prediction 

model : $Y=\beta_0+\beta_1x+\epsilon$

- $\beta_0+\beta_1x$ : first part is what we can determine (linear part). 

- $\epsilon$ : second part is something random, not in our control.

  - assume normal 

description : $\{x_i,y_i\}^n_{i=1}$

- $y_i=\hat{\beta}_0+\hat{\beta}_1x_i$
 
estimated model : $\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x$

point prediction :  $y_{\text{new}}=\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}+\epsilon_{\text{new}}$

- dont know random unknown term $\epsilon_{\text{new}}$

We can comput the CI of $\hat{\beta}_0$ and $\hat{\beta}_1$

- key part of this is that the $Var(\hat{\beta}_0)$ and $Var(\hat{\beta}_1)$

 Now want the CI of $y_{\text{new}}$
 
- we already know $\hat{y}_{\text{new}}=???$ this is our hat estimate

- next step is $Var(y_{\text{new}})$ 

  - if we can compute the variance of y hat then we can compute our CI
  
###

![](img/img4.png)

- this is a straight line 

- given $x_{\text{new}}$ we can find our prediction on this straight line 

So what is $y_{\text{new}}$

- it should be somewhere along the blue line 

- so we will find an interval that 95% of the data lies within, that we can way with 95% confidence that $y_{\text{new}}$ lies within 

- $Var(y_{\text{new}})$ is so important because it will help us with calculations from earlier in lecture (for upper and lower values : u and l) 

\begin{equation}\label{var of y hat}
\begin{split}
Var\{y_{\text{new}}\} & = Var\{\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}+\epsilon_{\text{new}}\}\\
& = Var\{\hat{\beta}_0+\hat{\beta}_1x_{\text{new}}\}+Var\{\epsilon_{\text{new}}\}\\
& = Var\{\hat{y}\}+\sigma^2
\end{split}
\end{equation}

What you need to do next is write out the formular for $\hat{y}$ and take the square root : $\sqrt{...\cdot\sigma^2}$

- will expand on the details on Monday 

- Homework expanded to Wednesday so we can do all predictions and confidence interval 

### Summary 

- will expand on prediction on monday 

For $Pr\{l<\beta_1<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\hat{\beta}_1+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

$\Rightarrow l=\hat{\beta}_1-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_1)$

For $Pr\{l<\beta_0<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\hat{\beta}_0+t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

$\Rightarrow l=\hat{\beta}_0-t_{\frac{\alpha}{2},n-2}\cdot s.e.(\hat{\beta}_0)$

For $Pr\{l<\sigma^2<u\}=1-\alpha$ the CI are : 

$\Rightarrow u=\frac{(n-2)MSE}{\chi^2_{\frac{\alpha}{2},n-2}}$

$\Rightarrow l=\frac{(n-2)MSE}{\chi^2_{1-\frac{\alpha}{2},n-2}}$

### To Do 

- rewatch mon. Oct. 4th lecture 

- prep homework 2 (complete)

- spruce up HW2 

# D10 : Mon. Oct. 15th, 2021

